{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "\"\"\"\n",
    "Some of code copy from\n",
    "https://github.com/openai/baselines/blob/master/baselines/ppo1/pposgd_simple.py\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "import gym, os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "import random, cv2\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "\n",
    "EPSILON = 0.2\n",
    "# 3 * 3 + 2\n",
    "ONE_HOT_SIZE = 10\n",
    "STATE_SIZE = 11\n",
    "F = STATE_SIZE + 1\n",
    "EMBED_SIZE = 5\n",
    "LAYER_SIZE = 128\n",
    "\n",
    "C = 1\n",
    "HERO_COUNT = 2\n",
    "# Hero skill mask, to indicate if a hero skill is a directional one.\n",
    "g_dir_skill_mask = [[True, False, False, False], [True, False, False, True]]\n",
    "\n",
    "NUM_FRAME_PER_ACTION = 4\n",
    "BATCH_SIZE = 4096*12\n",
    "EPOCH_NUM = 4\n",
    "LEARNING_RATE = 3e-2\n",
    "TIMESTEPS_PER_ACTOR_BATCH = 256*512\n",
    "GAMMA = 0.99\n",
    "LAMBDA = 0.95\n",
    "NUM_STEPS = 5000\n",
    "ENV_NAME = 'gym_moba:moba-multiplayer-v0'\n",
    "RANDOM_START_STEPS = 4\n",
    "\n",
    "global g_step\n",
    "\n",
    "# Generating data worker count\n",
    "g_data_generator_count = 3\n",
    "\n",
    "# Use hero id embedding\n",
    "g_embed_hero_id = False\n",
    "\n",
    "# Save model in pb format\n",
    "g_save_pb_model = False\n",
    "\n",
    "# Control if output to tensorboard\n",
    "g_out_tb = True\n",
    "\n",
    "# Control if train or play\n",
    "g_is_train = True\n",
    "# True means start a new train task without loading previous model.\n",
    "g_start_anew = True\n",
    "\n",
    "# Control if use priority sampling\n",
    "g_enable_per = False\n",
    "g_per_alpha = 0.6\n",
    "g_is_beta_start = 0.4\n",
    "g_is_beta_end = 1\n",
    "\n",
    "\n",
    "\n",
    "def stable_softmax(logits, name): \n",
    "    a = logits - tf.reduce_max(logits, axis=-1, keepdims=True) \n",
    "    ea = tf.exp(a) \n",
    "    z = tf.reduce_sum(ea, axis=-1, keepdims=True) \n",
    "    return tf.div(ea, z, name = name) \n",
    "\n",
    "class Environment(object):\n",
    "  def __init__(self):\n",
    "    self.env = gym.make(ENV_NAME)\n",
    "    self._screen = None\n",
    "    self.reward = 0\n",
    "    self.terminal = True\n",
    "    self.random_start = RANDOM_START_STEPS\n",
    "    self.obs = np.zeros(shape=(HERO_COUNT, F, C), dtype=np.float)\n",
    "\n",
    "  def get_action_number(self):\n",
    "    return 9\n",
    "\n",
    "  def step(self, action):\n",
    "    self._screen, self.reward, self.terminal, info = self.env.step(action)\n",
    "    self.obs[..., :-1] = self.obs[..., 1:]\n",
    "    self.obs[..., -1] = self._screen\n",
    "    return self.obs, self.reward, self.terminal, info\n",
    "\n",
    "  def reset(self):\n",
    "    self._screen = self.env.reset()\n",
    "    ob, _1, _2, _3 = self.step([[0, 0, 0], [0,0,0]])\n",
    "\n",
    "    return ob\n",
    "\n",
    "\n",
    "class MultiPlayer_Data_Generator():\n",
    "    def __init__(self, agent):\n",
    "        self.env = Environment()\n",
    "        self.agent = agent\n",
    "        self.timesteps_per_actor_batch = TIMESTEPS_PER_ACTOR_BATCH\n",
    "        self.seg_gen = self.traj_segment_generator(horizon=self.timesteps_per_actor_batch)\n",
    "    \n",
    "    def traj_segment_generator(self, horizon=256):\n",
    "        '''\n",
    "        horizon: int timesteps_per_actor_batch\n",
    "        '''\n",
    "        t = 0\n",
    "        ac = [[0, 0, 0], [0,0,0]]\n",
    "\n",
    "        new = True # marks if we're on first timestep of an episode\n",
    "        ob = self.env.reset()\n",
    "\n",
    "        cur_ep_ret = 0 # return in current episode\n",
    "        cur_ep_unclipped_ret = 0 # unclipped return in current episode\n",
    "        cur_ep_len = 0 # len of current episode\n",
    "        ep_rets = [] # returns of completed episodes in this segment\n",
    "        ep_unclipped_rets = [] # unclipped returns of completed episodes in this segment\n",
    "        ep_lens = [] # lengths of ...\n",
    "\n",
    "        # Initialize history arrays\n",
    "        obs = np.array([np.zeros(shape=(HERO_COUNT, F, C), dtype=np.float32) for _ in range(horizon)])\n",
    "        rews = np.zeros(horizon, 'float32')\n",
    "        unclipped_rews = np.zeros(horizon, 'float32')\n",
    "        vpreds = np.zeros(horizon, 'float32')\n",
    "        news = np.zeros(horizon, 'int32')\n",
    "        acs = np.array([ac for _ in range(horizon)])\n",
    "        prevacs = acs.copy()\n",
    "\n",
    "        while True:\n",
    "            prevac = ac\n",
    "            ac, vpred = self.agent.predict(ob[np.newaxis, ...])\n",
    "            #print('Action:', ac, 'Value:', vpred)\n",
    "            # Slight weirdness here because we need value function at time T\n",
    "            # before returning segment [0, T-1] so we get the correct\n",
    "            # terminal value\n",
    "            if t > 0 and t % horizon == 0:\n",
    "                yield {\"ob\" : obs, \"rew\" : rews, \"vpred\" : vpreds, \"new\" : news,\n",
    "                        \"ac\" : acs, \"prevac\" : prevacs, \"nextvpred\": vpred * (1 - new),\n",
    "                        \"ep_rets\" : ep_rets, \"ep_lens\" : ep_lens,\n",
    "                        \"ep_unclipped_rets\": ep_unclipped_rets}\n",
    "                # Be careful!!! if you change the downstream algorithm to aggregate\n",
    "                # several of these batches, then be sure to do a deepcopy\n",
    "                ep_rets = []\n",
    "                ep_unclipped_rets = []\n",
    "                ep_lens = []\n",
    "            i = t % horizon\n",
    "            obs[i] = ob\n",
    "            vpreds[i] = vpred\n",
    "            news[i] = new\n",
    "            acs[i] = ac\n",
    "            prevacs[i] = prevac\n",
    "\n",
    "            ob, unclipped_rew, new, step_info = self.env.step(ac)\n",
    "            rew = unclipped_rew\n",
    "            # rew = float(np.sign(unclipped_rew))\n",
    "            rews[i] = rew\n",
    "            unclipped_rews[i] = unclipped_rew\n",
    "\n",
    "            cur_ep_ret += rew\n",
    "            cur_ep_unclipped_ret += unclipped_rew\n",
    "            cur_ep_len += 1\n",
    "            if new or step_info > 600:\n",
    "                if False:#cur_ep_unclipped_ret == 0:\n",
    "                    pass\n",
    "                else:\n",
    "                    ep_rets.append(cur_ep_ret)\n",
    "                    ep_unclipped_rets.append(cur_ep_unclipped_ret)\n",
    "                    ep_lens.append(cur_ep_len)\n",
    "                cur_ep_ret = 0\n",
    "                cur_ep_unclipped_ret = 0\n",
    "                cur_ep_len = 0\n",
    "                ob = self.env.reset()\n",
    "            t += 1\n",
    "    \n",
    "    def add_vtarg_and_adv(self, seg, gamma=0.99, lam=0.95):\n",
    "        \"\"\"\n",
    "        Compute target value using TD(lambda) estimator, and advantage with GAE(lambda)\n",
    "        \"\"\"\n",
    "        new = np.append(seg[\"new\"], 0) # last element is only used for last vtarg, but we already zeroed it if last new = 1\n",
    "        vpred = np.append(seg[\"vpred\"], seg[\"nextvpred\"])\n",
    "        T = len(seg[\"rew\"])\n",
    "        seg[\"adv\"] = gaelam = np.empty(T, 'float32')\n",
    "        rew = seg[\"rew\"]\n",
    "        lastgaelam = 0\n",
    "        for t in reversed(range(T)):\n",
    "            nonterminal = 1-new[t+1]\n",
    "            delta = rew[t] + gamma * vpred[t+1] * nonterminal - vpred[t]\n",
    "            gaelam[t] = lastgaelam = delta + gamma * lam * nonterminal * lastgaelam\n",
    "        seg[\"tdlamret\"] = seg[\"adv\"] + seg[\"vpred\"]\n",
    "\n",
    "    def get_one_step_data(self):\n",
    "        seg = self.seg_gen.__next__()\n",
    "        self.add_vtarg_and_adv(seg, gamma=GAMMA, lam=LAMBDA)\n",
    "        ob, ac, atarg, tdlamret = seg[\"ob\"], seg[\"ac\"], seg[\"adv\"], seg[\"tdlamret\"]\n",
    "        if atarg.std() < 1e-5:\n",
    "            print('atarg std too small')\n",
    "        atarg = (atarg - atarg.mean()) / atarg.std() # standardized advantage function estimate\n",
    "        seg['std_atg'] = atarg\n",
    "        return ob, ac, atarg, tdlamret, seg\n",
    "class MultiPlayerAgent():\n",
    "\n",
    "    def __init__(self, session, a_space, a_space_keys, **options):       \n",
    "        self.importance_sample_arr = np.ones([TIMESTEPS_PER_ACTOR_BATCH], dtype=np.float)\n",
    "        self.session = session\n",
    "        # Here we get 4 action output heads\n",
    "        # 1 --- Meta action type\n",
    "        # 0 : stay where they are\n",
    "        # 1 : move\n",
    "        # 2 : normal attack\n",
    "        # 3 : skill 1\n",
    "        # 4 : skill 2\n",
    "        # 5 : skill 3\n",
    "        # 6 : extra skill\n",
    "\n",
    "        # Why we need seperate move direction and skill direction to two output heads??\n",
    "        # Answer: Help resolving sample independence.\n",
    "        # 2 --- Move direction, 8 directions, 0-7\n",
    "        # 3 --- Skill direction, 8 directions, 0-7\n",
    "        # 4 --- Skill target area, 0-8\n",
    "        self.a_space = a_space\n",
    "        self.a_space_keys = a_space_keys\n",
    "        self.policy_head_num = len(a_space)\n",
    "\n",
    "        self.input_dims = F\n",
    "        self.learning_rate = LEARNING_RATE\n",
    "        self.num_total_steps = NUM_STEPS\n",
    "        self.lenbuffer = deque(maxlen=100) # rolling buffer for episode lengths\n",
    "        self.rewbuffer = deque(maxlen=100) # rolling buffer for clipped episode rewards\n",
    "        self.unclipped_rewbuffer = deque(maxlen=100) # rolling buffer for unclipped episode rewards\n",
    "        \n",
    "        self.restrict_x_min = -0.5\n",
    "        self.restrict_x_max = 0.5\n",
    "        self.restrict_y_min = -0.5\n",
    "        self.restrict_y_max = 0.5\n",
    "\n",
    "        # Full connection layer node size\n",
    "        self.layer_size = LAYER_SIZE\n",
    "\n",
    "        self._init_input()\n",
    "        self._init_nn()\n",
    "        self._init_op()\n",
    "\n",
    "        self.session.run(tf.global_variables_initializer())        \n",
    "        \n",
    "\n",
    "    def _init_input(self, *args):\n",
    "        with tf.variable_scope('input'):\n",
    "            self.multi_s = tf.placeholder(tf.float32, [None, HERO_COUNT, self.input_dims, C], name='multi_s')\n",
    "\n",
    "            # Action shall have 2 elements, \n",
    "            self.multi_a = tf.placeholder(tf.int32, [None, HERO_COUNT, self.policy_head_num], name='multi_a')\n",
    "\n",
    "            self.cumulative_r = tf.placeholder(tf.float32, [None, ], name='cumulative_r')\n",
    "            self.adv = tf.placeholder(tf.float32, [None, ], name='adv')\n",
    "            self.importance_sample_arr_pl = tf.placeholder(tf.float32, [None, ], name='importance_sample')\n",
    "\n",
    "            tf.summary.histogram(\"input_state\", self.multi_s)\n",
    "            tf.summary.histogram(\"input_action\", self.multi_a)\n",
    "            tf.summary.histogram(\"input_cumulative_r\", self.cumulative_r)\n",
    "            tf.summary.histogram(\"input_adv\", self.adv)\n",
    "\n",
    "\n",
    "    def _init_nn(self, *args):\n",
    "        self.a_policy_new, self.a_policy_logits_new, self.value, self.summary_new = self._init_combine_actor_net('policy_net_new', HERO_COUNT, trainable=True)\n",
    "        self.a_policy_old, self.a_policy_logits_old, _, self.summary_old = self._init_combine_actor_net('policy_net_old', HERO_COUNT, trainable=False)\n",
    "\n",
    "    def _init_op(self):\n",
    "        self.lrmult = tf.placeholder(name='lrmult', dtype=tf.float32, shape=[])\n",
    "\n",
    "        with tf.variable_scope('update_target_actor_net'):\n",
    "            # Get eval w, b.\n",
    "            params_new = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='policy_net_new')\n",
    "            params_old = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='policy_net_old')\n",
    "            self.update_policy_net_op = [tf.assign(o, n) for o, n in zip(params_old, params_new)]\n",
    "\n",
    "        \n",
    "        with tf.variable_scope('critic_loss_func'):\n",
    "            # loss func.\n",
    "            #self.c_loss_func = tf.losses.mean_squared_error(labels=self.cumulative_r, predictions=self.value)\n",
    "            self.c_loss_func_arr = tf.square(sum(self.value) - self.cumulative_r)\n",
    "            self.c_loss_func = tf.reduce_mean(self.c_loss_func_arr)\n",
    "\n",
    "        with tf.variable_scope('actor_loss_func'):\n",
    "            batch_size = tf.shape(self.multi_a)[0]\n",
    "            a_loss_func_arr = tf.zeros([batch_size, ], dtype = tf.float32) \n",
    "            ratio = tf.ones([batch_size, ], dtype = tf.float32) \n",
    "            for hero_idx in range(HERO_COUNT):                \n",
    "                for idx in range(self.policy_head_num):\n",
    "                    a = self.multi_a[:,hero_idx, idx] #tf.slice(self.multi_a, [0, hero_idx, idx], [batch_size, 1, 1]) # a = \n",
    "                    a = tf.squeeze(a)\n",
    "                    a_mask_cond = tf.equal(a, -1)\n",
    "                    a = tf.where(a_mask_cond, tf.zeros([batch_size, ], dtype=tf.int32), a)\n",
    "                    # We correct -1 value in a, in order to make the condition operation below work\n",
    "                    a_indices = tf.stack([tf.range(batch_size, dtype=tf.int32), a], axis=1)\n",
    "                    \n",
    "                    new_policy_prob = tf.where(a_mask_cond, tf.ones([batch_size, ], dtype=tf.float32), tf.gather_nd(params=self.a_policy_new[hero_idx][idx], indices=a_indices))\n",
    "                    old_policy_prob = tf.where(a_mask_cond, tf.ones([batch_size, ], dtype=tf.float32), tf.gather_nd(params=self.a_policy_old[hero_idx][idx], indices=a_indices))\n",
    "\n",
    "                    ratio = tf.multiply(ratio, tf.exp(tf.log(new_policy_prob) - tf.log(old_policy_prob)))\n",
    "\n",
    "            surr = ratio * self.adv                                 # surrogate loss\n",
    "            a_loss_func_arr = tf.minimum(        # clipped surrogate objective\n",
    "                surr,\n",
    "                tf.clip_by_value(ratio, 1. - EPSILON*self.lrmult, 1. + EPSILON*self.lrmult) * self.adv)\n",
    "            self.a_loss_func = -tf.reduce_mean(a_loss_func_arr)\n",
    "\n",
    "        with tf.variable_scope('kl_distance'):\n",
    "            self.kl_distance = tf.zeros([1, ], dtype = tf.float32)\n",
    "            for hero_idx in range(HERO_COUNT):  \n",
    "                for idx in range(self.policy_head_num):\n",
    "                    a0 = self.a_policy_logits_new[hero_idx][idx] - tf.reduce_max(self.a_policy_logits_new[hero_idx][idx], axis=-1, keepdims=True)\n",
    "                    a1 = self.a_policy_logits_old[hero_idx][idx] - tf.reduce_max(self.a_policy_logits_old[hero_idx][idx], axis=-1, keepdims=True)\n",
    "                    ea0 = tf.exp(a0)\n",
    "                    ea1 = tf.exp(a1)\n",
    "                    z0 = tf.reduce_sum(ea0, axis=-1, keepdims=True)\n",
    "                    z1 = tf.reduce_sum(ea1, axis=-1, keepdims=True)\n",
    "                    p0 = ea0 / z0\n",
    "                    self.kl_distance += tf.reduce_sum(p0 * (a0 - tf.log(z0) - a1 + tf.log(z1)), axis=-1)\n",
    "\n",
    "        with tf.variable_scope('policy_entropy'):            \n",
    "            self.policy_entropy_arr = tf.zeros([BATCH_SIZE, ], dtype = tf.float32)\n",
    "            for hero_idx in range(HERO_COUNT):  \n",
    "                for idx in range(self.policy_head_num):\n",
    "                    a0 = self.a_policy_logits_new[hero_idx][idx] - tf.reduce_max(self.a_policy_logits_new[hero_idx][idx] , axis=-1, keepdims=True)\n",
    "                    ea0 = tf.exp(a0)\n",
    "                    z0 = tf.reduce_sum(ea0, axis=-1, keepdims=True)\n",
    "                    p0 = ea0 / z0\n",
    "                    self.policy_entropy_arr += tf.reduce_sum(p0 * (tf.log(z0) - a0), axis=-1)\n",
    "                \n",
    "            self.policy_entropy = tf.reduce_mean(self.policy_entropy_arr)\n",
    "\n",
    "        with tf.variable_scope('optimizer'):\n",
    "            if g_enable_per:          \n",
    "                self.total_loss_arr = -self.a_loss_func_arr + self.c_loss_func_arr - 0.01*self.policy_entropy_arr\n",
    "                self.total_loss_arr = tf.multiply(self.total_loss_arr, self.importance_sample_arr_pl)\n",
    "                self.total_loss = tf.reduce_mean(self.total_loss_arr)\n",
    "            else:\n",
    "                self.total_loss = self.a_loss_func + self.c_loss_func - 0.01*self.policy_entropy \n",
    "            '''\n",
    "            self.total_loss_arr = self.a_loss_func_arr + self.c_loss_func_arr - 0.01*self.policy_entropy_arr\n",
    "            self.total_loss = self.a_loss_func + self.c_loss_func - 0.01*self.policy_entropy \n",
    "            '''\n",
    "            learning_rate = self.learning_rate * self.lrmult\n",
    "            # Passing global_step to minimize() will increment it at each step.\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.total_loss)\n",
    "            #self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.total_loss)\n",
    "\n",
    "    def _init_combine_actor_net(self, scope, actor_count, trainable=True): \n",
    "        a_prob_arr_arr = []\n",
    "        a_logits_arr_arr = []\n",
    "        value_arr = []\n",
    "\n",
    "        with tf.variable_scope(scope):\n",
    "            for actor_idx in range(actor_count):\n",
    "                input_pl = self.multi_s[:,actor_idx,:,:]\n",
    "                # Share weights\n",
    "                a_prob_arr, a_logits_arr, value = self._init_single_actor_net(scope, input_pl, trainable)\n",
    "                a_prob_arr_arr.append(a_prob_arr)\n",
    "                a_logits_arr_arr.append(a_logits_arr)\n",
    "                value_arr.append(value)\n",
    "\n",
    "            merged_summary = tf.summary.merge_all()\n",
    "            return a_prob_arr_arr, a_logits_arr_arr, value_arr, merged_summary\n",
    "        pass\n",
    "\n",
    "    def _init_single_actor_net(self, scope, input_pl, trainable=True):        \n",
    "        my_initializer = tf.contrib.layers.xavier_initializer()\n",
    "        with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "            flat_output_size = F*C\n",
    "            flat_output = tf.reshape(input_pl, [-1, flat_output_size], name='flat_output')\n",
    "\n",
    "            # Add hero one-hot vector embedding\n",
    "            if g_embed_hero_id:\n",
    "                input_state = flat_output[:,:STATE_SIZE]\n",
    "                input_hero_id = flat_output[:,STATE_SIZE:STATE_SIZE + 1]\n",
    "                input_hero_one_hot = tf.one_hot(input_hero_id, ONE_HOT_SIZE)\n",
    "                fc_W_embed = tf.get_variable(shape=[ONE_HOT_SIZE, EMBED_SIZE], name='fc_W_embed',\n",
    "                    trainable=trainable, initializer=my_initializer)\n",
    "                fc_b_embed = tf.get_variable(shape=[EMBED_SIZE], name='fc_b_embed',\n",
    "                    trainable=trainable, initializer=my_initializer)\n",
    "\n",
    "                output_embedding = tf.nn.relu(tf.matmul(input_hero_one_hot, fc_W_embed) + fc_b_embed)\n",
    "\n",
    "                input_after_embed = tf.concat([input_state, output_embedding], -1)\n",
    "                fc_W_1 = tf.get_variable(shape=[EMBED_SIZE + STATE_SIZE, self.layer_size], name='fc_W_1',\n",
    "                    trainable=trainable, initializer=my_initializer)\n",
    "\n",
    "                fc_b_1 = tf.get_variable(shape=[self.layer_size], name='fc_b_1',\n",
    "                    trainable=trainable, initializer=my_initializer)                    \n",
    "\n",
    "                tf.summary.histogram(\"fc_W_1\", fc_W_1)\n",
    "                tf.summary.histogram(\"fc_b_1\", fc_b_1)\n",
    "\n",
    "                output1 = tf.nn.relu(tf.matmul(input_after_embed, fc_W_1) + fc_b_1)\n",
    "            else:\n",
    "                fc_W_1 = tf.get_variable(shape=[flat_output_size, self.layer_size], name='fc_W_1',\n",
    "                    trainable=trainable, initializer=my_initializer)\n",
    "\n",
    "                fc_b_1 = tf.get_variable(shape=[self.layer_size], name='fc_b_1',\n",
    "                    trainable=trainable, initializer=my_initializer)\n",
    "\n",
    "                tf.summary.histogram(\"fc_W_1\", fc_W_1)\n",
    "                tf.summary.histogram(\"fc_b_1\", fc_b_1)\n",
    "\n",
    "                output1 = tf.nn.relu(tf.matmul(flat_output, fc_W_1) + fc_b_1)                \n",
    "\n",
    "            fc_W_2 = tf.get_variable(shape=[self.layer_size, self.layer_size], name='fc_W_2',\n",
    "                trainable=trainable, initializer=my_initializer)\n",
    "\n",
    "            fc_b_2 = tf.get_variable(shape=[self.layer_size], name='fc_b_2',\n",
    "                trainable=trainable, initializer=my_initializer)\n",
    "\n",
    "            tf.summary.histogram(\"fc_W_2\", fc_W_2)\n",
    "            tf.summary.histogram(\"fc_b_2\", fc_b_2)\n",
    "\n",
    "            output2 = tf.nn.relu(tf.matmul(output1, fc_W_2) + fc_b_2)\n",
    "\n",
    "\n",
    "            fc_W_3 = tf.get_variable(shape=[self.layer_size, self.layer_size], name='fc_W_3',\n",
    "                trainable=trainable, initializer=my_initializer)\n",
    "\n",
    "            fc_b_3 = tf.get_variable(shape=[self.layer_size], name='fc_b_3',\n",
    "                trainable=trainable, initializer=my_initializer)\n",
    "\n",
    "            tf.summary.histogram(\"fc_W_3\", fc_W_3)\n",
    "            tf.summary.histogram(\"fc_b_3\", fc_b_3)\n",
    "\n",
    "            output3 = tf.nn.relu(tf.matmul(output2, fc_W_3) + fc_b_3)\n",
    "            a_logits_arr = []\n",
    "            a_prob_arr = []\n",
    "            #self.a_space_keys\n",
    "            for k in self.a_space_keys:\n",
    "                output_num = self.a_space[k]\n",
    "                # actor network\n",
    "                weight_layer_name = 'fc_W_{}'.format(k)\n",
    "                bias_layer_name = 'fc_b_{}'.format(k)\n",
    "                logit_layer_name = '{}_logits'.format(k)\n",
    "                head_layer_name = '{}_head'.format(k)\n",
    "\n",
    "                fc_W_a = tf.get_variable(shape=[self.layer_size, output_num], name=weight_layer_name,\n",
    "                    trainable=trainable, initializer=my_initializer)\n",
    "\n",
    "                fc_b_a = tf.get_variable(shape=[output_num], name=bias_layer_name,\n",
    "                    trainable=trainable, initializer=my_initializer)                    \n",
    "\n",
    "                tf.summary.histogram(weight_layer_name, fc_W_a)\n",
    "                tf.summary.histogram(bias_layer_name, fc_b_a)            \n",
    "\n",
    "                a_logits = tf.matmul(output3, fc_W_a) + fc_b_a\n",
    "                a_logits_arr.append(a_logits)                \n",
    "                tf.summary.histogram(logit_layer_name, a_logits)\n",
    "\n",
    "                a_prob = stable_softmax(a_logits, head_layer_name) #tf.nn.softmax(a_logits)\n",
    "                a_prob_arr.append(a_prob)\n",
    "                tf.summary.histogram(head_layer_name, a_prob)\n",
    "\n",
    "            # value network\n",
    "            fc1_W_v = tf.get_variable(shape=[self.layer_size, 1], name='fc1_W_v',\n",
    "                trainable=trainable, initializer=my_initializer)\n",
    "\n",
    "            fc1_b_v = tf.get_variable(shape=[1], name='fc1_b_v',\n",
    "                trainable=trainable, initializer=my_initializer)\n",
    "\n",
    "            tf.summary.histogram(\"fc1_W_v\", fc1_W_v)\n",
    "            tf.summary.histogram(\"fc1_b_v\", fc1_b_v)\n",
    "\n",
    "            value = tf.matmul(output3, fc1_W_v) + fc1_b_v\n",
    "            value = tf.reshape(value, [-1, ], name = \"value_output\")\n",
    "            tf.summary.histogram(\"value_head\", value)\n",
    "            \n",
    "            return a_prob_arr, a_logits_arr, value\n",
    "\n",
    "    def predict(self, s):\n",
    "        # Calculate a eval prob.\n",
    "        action_arr = []\n",
    "        value_arr = []\n",
    "\n",
    "        for hero_idx in range(HERO_COUNT):\n",
    "            tuple_val = self.session.run([self.value[hero_idx], self.a_policy_new[hero_idx][0], self.a_policy_new[hero_idx][1], self.a_policy_new[hero_idx][2]], feed_dict={self.multi_s: s})\n",
    "            value = tuple_val[0]\n",
    "            chosen_policy = tuple_val[1:]\n",
    "            #chosen_policy = self.session.run(self.a_policy_new, feed_dict={self.s: s})\n",
    "            \n",
    "            actions = []\n",
    "            for _idx in range(self.policy_head_num):\n",
    "                ac = np.random.choice(range(chosen_policy[_idx].shape[1]), p=chosen_policy[_idx][0])\n",
    "                actions.append(ac)\n",
    "\n",
    "            skill_is_dir = False\n",
    "            skill_dir_mask = g_dir_skill_mask[hero_idx]\n",
    "            check_if_skill = actions[0] >= 3 and actions[0] <= 6\n",
    "            if check_if_skill:\n",
    "                skill_is_dir = skill_dir_mask[actions[0] - 3]\n",
    "\n",
    "            if actions[0] == 0:\n",
    "                # Stay still\n",
    "                actions[1] = -1\n",
    "                actions[2] = -1\n",
    "            elif actions[0] == 1:\n",
    "                # Move\n",
    "                actions[2] = -1\n",
    "            elif actions[0] == 2:\n",
    "                # Normal attack\n",
    "                actions[1] = -1\n",
    "                actions[2] = -1\n",
    "            elif check_if_skill:\n",
    "                # skill 1 attack\n",
    "                actions[1] = -1\n",
    "                if not skill_is_dir:\n",
    "                    actions[2] = -1\n",
    "            else:\n",
    "                print('Action predict wrong:{}'.format(actions[0]))\n",
    "\n",
    "            action_arr.append(actions)\n",
    "            value_arr.append(value)\n",
    "\n",
    "        return action_arr, sum(value_arr)\n",
    "\n",
    "    def greedy_predict(self, s):\n",
    "        # Calculate a eval prob.\n",
    "        action_arr = []\n",
    "        value_arr = []\n",
    "\n",
    "        for idx in range(HERO_COUNT):\n",
    "            tuple_val = self.session.run([self.value[idx], self.a_policy_new[idx][0], self.a_policy_new[idx][1], self.a_policy_new[idx][2]], feed_dict={self.s: s[idx]})\n",
    "            value = tuple_val[0]\n",
    "            chosen_policy = tuple_val[1:]\n",
    "            #chosen_policy = self.session.run(self.a_policy_new, feed_dict={self.s: s})\n",
    "            actions = []\n",
    "            for idx in range(self.policy_head_num):            \n",
    "                ac = np.argmax(chosen_policy[idx][0])\n",
    "                actions.append(ac)\n",
    "            if actions[0] == 0:\n",
    "                # Stay still\n",
    "                actions[1] = -1\n",
    "                actions[2] = -1\n",
    "            elif actions[0] == 1:\n",
    "                # Move\n",
    "                actions[2] = -1\n",
    "            elif actions[0] == 2:\n",
    "                # Normal attack\n",
    "                actions[1] = -1\n",
    "                actions[2] = -1\n",
    "            elif actions[0] == 3:\n",
    "                # skill 1 attack\n",
    "                actions[1] = -1\n",
    "            #    actions[2] = -1\n",
    "            elif actions[0] == 4:\n",
    "                # skill 2 attack\n",
    "                actions[1] = -1\n",
    "                actions[2] = -1        \n",
    "            elif actions[0] == 5:\n",
    "                # skill 3 attack\n",
    "                actions[1] = -1\n",
    "                actions[2] = -1\n",
    "            elif actions[0] == 6:\n",
    "                # skill 4 attack\n",
    "                actions[1] = -1\n",
    "            #    actions[2] = -1 \n",
    "            else:\n",
    "                print('Action predict wrong:{}'.format(actions[0]))\n",
    "            action_arr.append(actions)\n",
    "            value_arr.append(value)\n",
    "\n",
    "        return action_arr, value_arr\n",
    "\n",
    "    def learn_one_traj(self, timestep, ob, ac, atarg, tdlamret, lens, rets, unclipped_rets, train_writer):\n",
    "        global g_step\n",
    "        self.session.run(self.update_policy_net_op)\n",
    "\n",
    "        lrmult = max(1.0 - float(timestep) / self.num_total_steps, .0)\n",
    "\n",
    "        Entropy_list = []\n",
    "        KL_distance_list = []\n",
    "        atarg = np.array(atarg)\n",
    "        ob = np.array(ob)\n",
    "        ac = np.array(ac)\n",
    "        tdlamret = np.array(tdlamret)\n",
    "        \n",
    "        for _idx in range(EPOCH_NUM):\n",
    "            indices = np.random.permutation(len(ob)).tolist()\n",
    "            inner_loop_count = (len(ob)//BATCH_SIZE)\n",
    "            for i in range(inner_loop_count):\n",
    "                temp_indices = indices[i*BATCH_SIZE : (i+1)*BATCH_SIZE]               \n",
    "\n",
    "             #   print('temp_indices len:{}, shape:{}, adv len:{}'.format(type(temp_indices), temp_indices[:20], type(atarg)))\n",
    "                # Minimize loss.\n",
    "                _, entropy, kl_distance, summary_new_val, summary_old_val = self.session.run([self.optimizer, self.policy_entropy, self.kl_distance, self.summary_new, self.summary_old], {\n",
    "                    self.lrmult : lrmult,\n",
    "                    self.adv: atarg[temp_indices],\n",
    "                    self.multi_s: ob[temp_indices],\n",
    "                    self.multi_a: ac[temp_indices],\n",
    "                    self.cumulative_r: tdlamret[temp_indices],\n",
    "                })\n",
    "                \n",
    "                if g_out_tb and i == (inner_loop_count - 1) and _idx == (EPOCH_NUM -1):\n",
    "                    g_step += 1\n",
    "                    #train_writer.add_summary(summary_new_val, g_step)\n",
    "                    #train_writer.add_summary(summary_old_val, g_step)\n",
    "\n",
    "                Entropy_list.append(entropy)\n",
    "                KL_distance_list.append(kl_distance)\n",
    "\n",
    "        self.lenbuffer.extend(lens)\n",
    "        self.rewbuffer.extend(rets)\n",
    "        self.unclipped_rewbuffer.extend(unclipped_rets)\n",
    "        return np.mean(Entropy_list), np.mean(KL_distance_list)\n",
    "\n",
    "def get_one_step_data(timestep, work_thread_count):\n",
    "    root_folder = os.path.split(os.path.abspath(__file__))[0]\n",
    "    ob, ac, std_atvtg, tdlamret, lens, rets, unclipped_rets = [], [], [], [], [], [], []\n",
    "    # Enumerate data files under folder\n",
    "    data_folder_path = '{}/../distribute_collected_train_data/{}'.format(root_folder, timestep)\n",
    "    collected_all_data_files = False\n",
    "    while not collected_all_data_files:\n",
    "        for root, _, files in os.walk(data_folder_path):\n",
    "            if len(files) < work_thread_count:\n",
    "                print('Already has {} files, waiting for the worker thread generate more data files.'.format(len(files)))\n",
    "                break\n",
    "\n",
    "            for file_name in files:\n",
    "                full_file_name = '{}/{}'.format(root, file_name)\n",
    "                with open(full_file_name, 'rb') as file_handle:\n",
    "                    _seg = pickle.load(file_handle)\n",
    "                    ob.extend(_seg[\"ob\"])\n",
    "                    ac.extend(_seg[\"ac\"])\n",
    "                    std_atvtg.extend(_seg[\"std_atvtg\"])\n",
    "                    tdlamret.extend(_seg[\"tdlamret\"])\n",
    "                    lens.extend(_seg[\"ep_lens\"])\n",
    "                    rets.extend(_seg[\"ep_rets\"])\n",
    "                    unclipped_rets.extend(_seg[\"ep_unclipped_rets\"])                    \n",
    "\n",
    "            collected_all_data_files = True\n",
    "            break        \n",
    "        time.sleep(2)\n",
    "    print('Successfully collected {} files, data size:{}.'.format(len(files), len(ob)))\n",
    "    return ob, ac, std_atvtg, tdlamret, lens, rets, unclipped_rets\n",
    "\n",
    "\n",
    "def learn(num_steps=NUM_STEPS):\n",
    "    root_folder = os.path.split(os.path.abspath(__file__))[0]\n",
    "    global g_step\n",
    "    g_step = 0\n",
    "    session = tf.Session()\n",
    "\n",
    "    action_space_map = {'action':7, 'move':8, 'skill':8}\n",
    "    a_space_keys = ['action', 'move', 'skill']\n",
    "    agent = MultiPlayerAgent(session, action_space_map, a_space_keys)\n",
    "    train_writer = tf.summary.FileWriter('summary_log_gerry', graph=tf.get_default_graph()) \n",
    "    saver = tf.train.Saver(max_to_keep=1)\n",
    "    max_rew = -10000\n",
    "\n",
    "    for timestep in range(num_steps):\n",
    "        ob, ac, atarg, tdlamret, lens, rets, unclipped_rets = get_one_step_data(timestep, g_data_generator_count)\n",
    "\n",
    "        #entropy, kl_distance = agent.learn_one_traj(timestep, ob, ac, atarg, tdlamret, lens, rets, unclipped_rets, train_writer)\n",
    "\n",
    "        max_rew = max(max_rew, np.max(agent.unclipped_rewbuffer))\n",
    "\n",
    "        #saver.save(session, '{}/../ckpt/mnist.ckpt'.format(root_folder), global_step=timestep + 1)\n",
    "\n",
    "        #summary0 = tf.Summary()\n",
    "        #summary0.value.add(tag='EpLenMean', simple_value=np.mean(agent.lenbuffer))\n",
    "        #train_writer.add_summary(summary0, g_step)\n",
    "\n",
    "        #summary1 = tf.Summary()\n",
    "        #summary1.value.add(tag='UnClippedEpRewMean', simple_value=np.mean(agent.unclipped_rewbuffer))\n",
    "        #train_writer.add_summary(summary1, g_step)\n",
    "\n",
    "        print('Timestep:', timestep,\n",
    "            \"\\tEpLenMean:\", '%.3f'%np.mean(agent.lenbuffer),\n",
    "            \"\\tEpRewMean:\", '%.3f'%np.mean(agent.rewbuffer),\n",
    "            \"\\tUnClippedEpRewMean:\", '%.3f'%np.mean(agent.unclipped_rewbuffer),\n",
    "            \"\\tMaxUnClippedRew:\", max_rew)\n",
    "       #     \"\\tEntropy:\", '%.3f'%entropy,\n",
    "       #     \"\\tKL_distance:\", '%.8f'%kl_distance)\n",
    "\n",
    "def play_game():\n",
    "    session = tf.Session()\n",
    "    action_space_map = {'action':7, 'move':8, 'skill':8}\n",
    "    a_space_keys = ['action', 'move', 'skill']\n",
    "    agent = Agent(session, action_space_map, a_space_keys)\n",
    "\n",
    "    saver = tf.train.Saver(max_to_keep=1)\n",
    "    model_file=tf.train.latest_checkpoint('ckpt/')\n",
    "    if model_file != None:\n",
    "        saver.restore(session, model_file)\n",
    "\n",
    "    env = Environment()\n",
    "    \n",
    "    ob = env.reset()\n",
    "\n",
    "    while True:\n",
    "        time.sleep(0.2)\n",
    "        ac, _ = agent.greedy_predict(ob[np.newaxis, ...])\n",
    "        print('Predict :{}'.format(ac))\n",
    "\n",
    "        ob, unclipped_rew, new, _ = env.step(ac)\n",
    "        if new:\n",
    "            print('Game is finishd, reward is:{}'.format(unclipped_rew))\n",
    "            ob = env.reset()\n",
    "\n",
    "    pass\n",
    "\n",
    "def play_game_with_saved_model():\n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        tf.saved_model.loader.load(sess, [\"serve\"], \"./model/model_{}\".format(328704))\n",
    "        input_tensor = sess.graph.get_tensor_by_name('input/s:0')\n",
    "        policy_tensor = sess.graph.get_tensor_by_name('policy_net_new/soft_logits:0')\n",
    "        value_tensor = sess.graph.get_tensor_by_name('policy_net_new/value_output:0')\n",
    "\n",
    "        env = Environment()        \n",
    "        ob = env.reset()\n",
    "\n",
    "        while True:\n",
    "            time.sleep(0.05)\n",
    "\n",
    "            chosen_policy, _ = sess.run([policy_tensor, value_tensor], feed_dict={input_tensor: ob[np.newaxis, ...]})\n",
    "            tac = np.argmax(chosen_policy[0]) \n",
    "\n",
    "            #print('Predict :{}, input:{}, output:{}'.format(tac, ob, chosen_policy))\n",
    "\n",
    "            ob, reward, new, _ = env.step(tac)\n",
    "            if new:\n",
    "                print('Game is finishd, reward is:{}'.format(reward))\n",
    "                ob = env.reset()\n",
    "\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    bb = {1:'a', 2:'b', 3:'c'}\n",
    "    if len(sys.argv) > 1:\n",
    "        g_data_generator_count = int(sys.argv[1])\n",
    "\n",
    "    #print(list(bb))\n",
    "    a = list(map(lambda x: math.pow(x, -2), range(1, 10)))\n",
    "    for val in a:\n",
    "        print(val)\n",
    "    print(a)\n",
    "\n",
    "    try:\n",
    "        # Write control file\n",
    "        root_folder = os.path.split(os.path.abspath(__file__))[0]\n",
    "        ctrl_file_path = '{}/ctrl.txt'.format(root_folder)\n",
    "        file_handle = open(ctrl_file_path, 'w')\n",
    "        if g_is_train:\n",
    "            file_handle.write('1')\n",
    "        else:\n",
    "            file_handle.write('0')\n",
    "\n",
    "        file_handle.close()\n",
    "    except:\n",
    "        pass\t  \n",
    "\n",
    "    if g_is_train:\n",
    "        learn(num_steps=5000)\n",
    "    else:\n",
    "        play_game()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
